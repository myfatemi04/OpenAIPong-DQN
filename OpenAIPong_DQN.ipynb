{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "q4AS7njD7iL6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /home/michaelfatemi/.local/lib/python3.8/site-packages (4.7.0.72)\n",
            "Requirement already satisfied: torch in /home/michaelfatemi/.local/lib/python3.8/site-packages (2.0.1)\n",
            "Collecting gymnasium[accept-rom-license]\n",
            "  Using cached gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "Collecting ale-py\n",
            "  Using cached ale_py-0.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from gym) (6.6.0)\n",
            "Collecting cloudpickle>=1.2.0\n",
            "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from gym) (1.24.3)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Requirement already satisfied: sympy in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: filelock in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: networkx in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: jinja2 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: wheel in /home/michaelfatemi/.conda/envs/py38/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
            "Requirement already satisfied: setuptools in /home/michaelfatemi/.conda/envs/py38/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.8.0)\n",
            "Requirement already satisfied: cmake in /home/michaelfatemi/.local/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
            "Requirement already satisfied: lit in /home/michaelfatemi/.local/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Collecting jax-jumpy>=1.0.0\n",
            "  Using cached jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /home/michaelfatemi/.local/lib/python3.8/site-packages (from ale-py) (5.12.0)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0\n",
            "  Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: click in /home/michaelfatemi/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.3)\n",
            "Collecting requests\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: tqdm in /home/michaelfatemi/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.65.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Using cached AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/michaelfatemi/.conda/envs/py38/lib/python3.8/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/michaelfatemi/.local/lib/python3.8/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.1.0)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827636 sha256=2d50d61c274b95b73e141bf13c469272f961d7f7c25c976a031bee372530fce3\n",
            "  Stored in directory: /home/michaelfatemi/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=10feaeec4c808dbab0205fb6811bdfaedc9313d95b95f376bd72e49702c03d2a\n",
            "  Stored in directory: /home/michaelfatemi/.cache/pip/wheels/84/55/09/367b3a18e3c0cf9c69af93d77818a6ea19493121c65af499f6\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym-notices, farama-notifications, urllib3, jax-jumpy, cloudpickle, certifi, requests, gymnasium, gym, ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 certifi-2023.5.7 cloudpickle-2.2.1 farama-notifications-0.0.4 gym-0.26.2 gym-notices-0.0.8 gymnasium-0.28.1 jax-jumpy-1.0.0 requests-2.31.0 shimmy-0.2.1 urllib3-2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gym opencv-python torch \"gymnasium[accept-rom-license]\" ale-py \"gymnasium[atari]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/michaelfatemi/.conda/envs/py38/bin/pip\n"
          ]
        }
      ],
      "source": [
        "!which pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-D98CddQuwKG"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'gymnasium' from '/Users/michaelfatemi/miniconda3/envs/cs224n/lib/python3.8/site-packages/gymnasium/__init__.py'>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "importlib.reload(gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I8rGMlN2uzgk"
      },
      "outputs": [],
      "source": [
        "# ENVIRONMENT = \"PongDeterministic-v4\"\n",
        "ENVIRONMENT = \"ALE/Pong-v5\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SAVE_MODELS = True  # Save models to file so you can test later\n",
        "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
        "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
        "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
        "\n",
        "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
        "LOAD_FILE_EPISODE = 0  # Load Xth episode from file\n",
        "\n",
        "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
        "MAX_EPISODE = 100000  # Max episode\n",
        "MAX_STEP = 100000  # Max step size for one episode\n",
        "\n",
        "MAX_MEMORY_LEN = 50000  # Max memory len\n",
        "MIN_MEMORY_LEN = 40000  # Min memory len before start train\n",
        "\n",
        "GAMMA = 0.97  # Discount rate\n",
        "ALPHA = 0.00025  # Learning rate\n",
        "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
        "\n",
        "RENDER_GAME_WINDOW = False  # Opens a new window to render the game (Won't work on colab default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HxF5-bzUu1q-"
      },
      "outputs": [],
      "source": [
        "class DuelCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
        "    \"\"\"\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(DuelCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "\n",
        "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
        "\n",
        "        # Action layer\n",
        "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
        "\n",
        "        # State Value layer\n",
        "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        \"\"\"\n",
        "        Calcs conv layers output image sizes\n",
        "        \"\"\"\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
        "\n",
        "        Ax = self.Alrelu(self.Alinear1(x))\n",
        "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
        "\n",
        "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
        "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
        "\n",
        "        q = Vx + (Ax - Ax.mean())\n",
        "\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "plT51MPbu5U5"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"\n",
        "        Hyperparameters definition for Agent\n",
        "        \"\"\"\n",
        "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
        "        self.state_size_h = environment.observation_space.shape[0]\n",
        "        self.state_size_w = environment.observation_space.shape[1]\n",
        "        self.state_size_c = environment.observation_space.shape[2]\n",
        "\n",
        "        # Activation size for breakout env. Used as output size in network\n",
        "        self.action_size = environment.action_space.n\n",
        "\n",
        "        # Image pre process params\n",
        "        self.target_h = 80  # Height after process\n",
        "        self.target_w = 64  # Widht after process\n",
        "\n",
        "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
        "\n",
        "        # Trust rate to our experiences\n",
        "        self.gamma = GAMMA  # Discount coef for future predictions\n",
        "        self.alpha = ALPHA  # Learning Rate\n",
        "\n",
        "        # After many experinces epsilon will be 0.05\n",
        "        # So we will do less Explore more Exploit\n",
        "        self.epsilon = 1  # Explore or Exploit\n",
        "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
        "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
        "\n",
        "        # Deque holds replay mem.\n",
        "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
        "\n",
        "        # Create two model for DDQN algorithm\n",
        "        self.online_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # Adam used as optimizer\n",
        "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
        "\n",
        "    def preProcess(self, image):\n",
        "        \"\"\"\n",
        "        Process image crop resize, grayscale and normalize the images\n",
        "        \"\"\"\n",
        "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
        "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
        "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
        "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Get state and do action\n",
        "        Two option can be selectedd if explore select random action\n",
        "        if exploit ask nnet for action\n",
        "        \"\"\"\n",
        "\n",
        "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
        "\n",
        "        if act_protocol == 'Explore':\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
        "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
        "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
        "\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train neural nets with replay memory\n",
        "        returns loss and max_q val predicted from online_net\n",
        "        \"\"\"\n",
        "        if len(self.memory) < MIN_MEMORY_LEN:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "        # We get out minibatch and turn it to numpy array\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
        "\n",
        "        # Concat batches in one array\n",
        "        # (np.arr, np.arr) ==> np.BIGarr\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        # Convert them to tensors\n",
        "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
        "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        # Make predictions\n",
        "        state_q_values = self.online_model(state)\n",
        "        next_states_q_values = self.online_model(next_state)\n",
        "        next_states_target_q_values = self.target_model(next_state)\n",
        "\n",
        "        # Find selected action's q_value\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "        # Get indice of the max value of next_states_q_values\n",
        "        # Use that indice to get a q_value from next_states_target_q_values\n",
        "        # We use greedy for policy So it called off-policy\n",
        "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "        # Use Bellman function to find expected q value\n",
        "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
        "\n",
        "        # Calc loss with expected_q_value and q_value\n",
        "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, torch.max(state_q_values).item()\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        \"\"\"\n",
        "        Store every result to memory\n",
        "        \"\"\"\n",
        "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def adaptiveEpsilon(self):\n",
        "        \"\"\"\n",
        "        Adaptive Epsilon means every step\n",
        "        we decrease the epsilon so we do less Explore\n",
        "        \"\"\"\n",
        "        if self.epsilon > self.epsilon_minimum:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ve4vYDe3bozg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:1 Time:14:40:18 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.000 Avg_Max_Q:0.000 Epsilon:1.00 Duration:1.07 Step:956 CStep:958\n",
            "Episode:2 Time:14:41:16 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.000 Avg_Max_Q:0.000 Epsilon:0.99 Duration:57.56 Step:841 CStep:1800\n",
            "Episode:3 Time:14:41:16 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.000 Avg_Max_Q:0.000 Epsilon:0.98 Duration:0.74 Step:919 CStep:2720\n",
            "Episode:4 Time:14:41:17 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.000 Avg_Max_Q:0.000 Epsilon:0.97 Duration:0.72 Step:903 CStep:3624\n",
            "Episode:5 Time:14:41:18 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.200 Avg_Max_Q:0.000 Epsilon:0.96 Duration:0.63 Step:763 CStep:4388\n",
            "Episode:6 Time:14:41:18 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.333 Avg_Max_Q:0.000 Epsilon:0.95 Duration:0.70 Step:851 CStep:5240\n",
            "Episode:7 Time:14:41:19 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.429 Avg_Max_Q:0.000 Epsilon:0.94 Duration:0.75 Step:912 CStep:6153\n",
            "Episode:8 Time:14:41:20 Reward:-19.00 Loss:0.00 Last_100_Avg_Rew:-20.250 Avg_Max_Q:0.000 Epsilon:0.93 Duration:0.95 Step:1134 CStep:7288\n",
            "Episode:9 Time:14:41:21 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.333 Avg_Max_Q:0.000 Epsilon:0.92 Duration:0.72 Step:850 CStep:8139\n",
            "Episode:10 Time:14:41:22 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.300 Avg_Max_Q:0.000 Epsilon:0.91 Duration:0.75 Step:870 CStep:9010\n",
            "Episode:11 Time:14:41:22 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.364 Avg_Max_Q:0.000 Epsilon:0.91 Duration:0.75 Step:851 CStep:9862\n",
            "Episode:12 Time:14:41:23 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.333 Avg_Max_Q:0.000 Epsilon:0.90 Duration:0.74 Step:871 CStep:10734\n",
            "Episode:13 Time:14:41:24 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.385 Avg_Max_Q:0.000 Epsilon:0.90 Duration:0.68 Step:763 CStep:11498\n",
            "Episode:14 Time:14:41:25 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.357 Avg_Max_Q:0.000 Epsilon:0.89 Duration:0.77 Step:842 CStep:12341\n",
            "Episode:15 Time:14:41:26 Reward:-19.00 Loss:0.00 Last_100_Avg_Rew:-20.267 Avg_Max_Q:0.000 Epsilon:0.88 Duration:0.85 Step:964 CStep:13306\n",
            "Episode:16 Time:14:41:26 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.312 Avg_Max_Q:0.000 Epsilon:0.87 Duration:0.67 Step:763 CStep:14070\n",
            "Episode:17 Time:14:41:27 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.353 Avg_Max_Q:0.000 Epsilon:0.87 Duration:0.69 Step:763 CStep:14834\n",
            "Episode:18 Time:14:41:28 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.389 Avg_Max_Q:0.000 Epsilon:0.86 Duration:0.69 Step:763 CStep:15598\n",
            "Episode:19 Time:14:41:28 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.421 Avg_Max_Q:0.000 Epsilon:0.85 Duration:0.70 Step:763 CStep:16362\n",
            "Episode:20 Time:14:41:29 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.450 Avg_Max_Q:0.000 Epsilon:0.84 Duration:0.71 Step:763 CStep:17126\n",
            "Episode:21 Time:14:41:30 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.476 Avg_Max_Q:0.000 Epsilon:0.84 Duration:0.74 Step:763 CStep:17890\n",
            "Episode:22 Time:14:41:31 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.500 Avg_Max_Q:0.000 Epsilon:0.83 Duration:0.76 Step:763 CStep:18654\n",
            "Episode:23 Time:14:41:32 Reward:-18.00 Loss:0.00 Last_100_Avg_Rew:-20.391 Avg_Max_Q:0.000 Epsilon:0.83 Duration:1.00 Step:1059 CStep:19714\n",
            "Episode:24 Time:14:41:32 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.417 Avg_Max_Q:0.000 Epsilon:0.82 Duration:0.92 Step:945 CStep:20660\n",
            "Episode:25 Time:14:41:33 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.440 Avg_Max_Q:0.000 Epsilon:0.81 Duration:0.88 Step:883 CStep:21544\n",
            "Episode:26 Time:14:41:34 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.423 Avg_Max_Q:0.000 Epsilon:0.80 Duration:0.85 Step:872 CStep:22417\n",
            "Episode:27 Time:14:41:35 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.444 Avg_Max_Q:0.000 Epsilon:0.79 Duration:0.76 Step:763 CStep:23181\n",
            "Episode:28 Time:14:41:36 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.464 Avg_Max_Q:0.000 Epsilon:0.79 Duration:0.77 Step:763 CStep:23945\n",
            "Episode:29 Time:14:41:37 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.483 Avg_Max_Q:0.000 Epsilon:0.79 Duration:0.82 Step:823 CStep:24769\n",
            "Episode:30 Time:14:41:37 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.500 Avg_Max_Q:0.000 Epsilon:0.78 Duration:0.78 Step:763 CStep:25533\n",
            "Episode:31 Time:14:41:38 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.516 Avg_Max_Q:0.000 Epsilon:0.77 Duration:0.82 Step:763 CStep:26297\n",
            "Episode:32 Time:14:41:39 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.531 Avg_Max_Q:0.000 Epsilon:0.76 Duration:0.78 Step:763 CStep:27061\n",
            "Episode:33 Time:14:41:40 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.545 Avg_Max_Q:0.000 Epsilon:0.76 Duration:0.77 Step:763 CStep:27825\n",
            "Episode:34 Time:14:41:40 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.559 Avg_Max_Q:0.000 Epsilon:0.75 Duration:0.80 Step:782 CStep:28608\n",
            "Episode:35 Time:14:41:41 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.571 Avg_Max_Q:0.000 Epsilon:0.75 Duration:0.83 Step:823 CStep:29432\n",
            "Episode:36 Time:14:41:42 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.583 Avg_Max_Q:0.000 Epsilon:0.74 Duration:0.78 Step:763 CStep:30196\n",
            "Episode:37 Time:14:41:43 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.595 Avg_Max_Q:0.000 Epsilon:0.74 Duration:0.81 Step:763 CStep:30960\n",
            "Episode:38 Time:14:41:44 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.605 Avg_Max_Q:0.000 Epsilon:0.73 Duration:0.82 Step:763 CStep:31724\n",
            "Episode:39 Time:14:41:45 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.615 Avg_Max_Q:0.000 Epsilon:0.72 Duration:0.84 Step:763 CStep:32488\n",
            "Episode:40 Time:14:41:45 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.625 Avg_Max_Q:0.000 Epsilon:0.72 Duration:0.82 Step:763 CStep:33252\n",
            "Episode:41 Time:14:41:46 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.634 Avg_Max_Q:0.000 Epsilon:0.71 Duration:0.87 Step:823 CStep:34076\n",
            "Episode:42 Time:14:41:47 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.643 Avg_Max_Q:0.000 Epsilon:0.71 Duration:0.81 Step:763 CStep:34840\n",
            "Episode:43 Time:14:41:48 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.651 Avg_Max_Q:0.000 Epsilon:0.70 Duration:0.81 Step:763 CStep:35604\n",
            "Episode:44 Time:14:41:49 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.659 Avg_Max_Q:0.000 Epsilon:0.70 Duration:0.96 Step:885 CStep:36490\n",
            "Episode:45 Time:14:41:50 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.667 Avg_Max_Q:0.000 Epsilon:0.69 Duration:0.85 Step:763 CStep:37254\n",
            "Episode:46 Time:14:41:51 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.674 Avg_Max_Q:0.000 Epsilon:0.68 Duration:0.84 Step:763 CStep:38018\n",
            "Episode:47 Time:14:41:51 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.681 Avg_Max_Q:0.000 Epsilon:0.68 Duration:0.84 Step:763 CStep:38782\n",
            "Episode:48 Time:14:41:52 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.688 Avg_Max_Q:0.000 Epsilon:0.68 Duration:0.85 Step:763 CStep:39546\n",
            "Episode:49 Time:14:42:09 Reward:-21.00 Loss:16.66 Last_100_Avg_Rew:-20.694 Avg_Max_Q:0.077 Epsilon:0.67 Duration:16.56 Step:880 CStep:40427\n",
            "Episode:50 Time:14:42:22 Reward:-21.00 Loss:4.33 Last_100_Avg_Rew:-20.700 Avg_Max_Q:0.159 Epsilon:0.66 Duration:12.89 Step:763 CStep:41191\n"
          ]
        }
      ],
      "source": [
        "environment = gym.make(ENVIRONMENT)  # Get env\n",
        "agent = Agent(environment)  # Create Agent\n",
        "\n",
        "if LOAD_MODEL_FROM_FILE:\n",
        "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
        "\n",
        "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
        "        param = json.load(outfile)\n",
        "        agent.epsilon = param.get('epsilon')\n",
        "\n",
        "    startEpisode = LOAD_FILE_EPISODE + 1\n",
        "\n",
        "else:\n",
        "    startEpisode = 1\n",
        "\n",
        "last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
        "total_step = 1  # Cumulkative sum of all steps in episodes\n",
        "for episode in range(startEpisode, MAX_EPISODE):\n",
        "\n",
        "    startTime = time.time()  # Keep time\n",
        "    state = environment.reset()[0]  # Reset env\n",
        "\n",
        "    state = agent.preProcess(state)  # Process image\n",
        "\n",
        "    # Stack state . Every state contains 4 time contionusly frames\n",
        "    # We stack frames like 4 channel image\n",
        "    state = np.stack((state, state, state, state))\n",
        "\n",
        "    total_max_q_val = 0  # Total max q vals\n",
        "    total_reward = 0  # Total reward for each episode\n",
        "    total_loss = 0  # Total loss for each episode\n",
        "    for step in range(MAX_STEP):\n",
        "\n",
        "        if RENDER_GAME_WINDOW:\n",
        "            environment.render()  # Show state visually\n",
        "\n",
        "        # Select and perform an action\n",
        "        action = agent.act(state)  # Act\n",
        "        next_state, reward, terminated, truncated, info = environment.step(action)  # Observe\n",
        "\n",
        "        next_state = agent.preProcess(next_state)  # Process image\n",
        "\n",
        "        # Stack state . Every state contains 4 time contionusly frames\n",
        "        # We stack frames like 4 channel image\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state  # Update state\n",
        "\n",
        "        if TRAIN_MODEL:\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            loss, max_q_val = agent.train()  # Train with random BATCH_SIZE state taken from mem\n",
        "        else:\n",
        "            loss, max_q_val = [0, 0]\n",
        "\n",
        "        total_loss += loss\n",
        "        total_max_q_val += max_q_val\n",
        "        total_reward += reward\n",
        "        total_step += 1\n",
        "        if total_step % 1000 == 0:\n",
        "            agent.adaptiveEpsilon()  # Decrase epsilon\n",
        "\n",
        "        if done:  # Episode completed\n",
        "            currentTime = time.time()  # Keep current time\n",
        "            time_passed = currentTime - startTime  # Find episode duration\n",
        "            current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
        "            epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
        "\n",
        "            if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
        "                weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
        "                epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
        "\n",
        "                torch.save(agent.online_model.state_dict(), weightsPath)\n",
        "                with open(epsilonPath, 'w') as outfile:\n",
        "                    json.dump(epsilonDict, outfile)\n",
        "\n",
        "            if TRAIN_MODEL:\n",
        "                agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
        "\n",
        "            last_100_ep_reward.append(total_reward)\n",
        "            avg_max_q_val = total_max_q_val / step\n",
        "\n",
        "            outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
        "                episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
        "            )\n",
        "\n",
        "            print(outStr)\n",
        "\n",
        "            if SAVE_MODELS:\n",
        "                outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
        "                with open(outputPath, 'a') as outfile:\n",
        "                    outfile.write(outStr+\"\\n\")\n",
        "\n",
        "            break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "OpenAIPong-DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
